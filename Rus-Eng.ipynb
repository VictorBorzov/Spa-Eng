{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Spa-Eng.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"_330E6Iawtw-","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"Fv4q_TtCZlrg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":72},"outputId":"7afe4276-8844-4f21-e604-d1bbc5ac1a37","executionInfo":{"status":"ok","timestamp":1527363362693,"user_tz":-180,"elapsed":435935,"user":{"displayName":"Виктор Кириллович Борзов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102444268362895471857"}}},"cell_type":"code","source":["##########-DATASET-##########\n","from google.colab import files\n","uploaded = files.upload() "],"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-58194ca7-eaeb-4e71-9db4-8222e6ef5159\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-58194ca7-eaeb-4e71-9db4-8222e6ef5159\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving rus.txt to rus (1).txt\n"],"name":"stdout"}]},{"metadata":{"id":"9cy52SH51DCK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":2239},"outputId":"a0c04250-56dc-4800-e526-2ef79adba635","executionInfo":{"status":"ok","timestamp":1527364937599,"user_tz":-180,"elapsed":15543,"user":{"displayName":"Виктор Кириллович Борзов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102444268362895471857"}}},"cell_type":"code","source":["import string\n","import re\n","from pickle import dump\n","from unicodedata import normalize\n","from numpy import array\n","\n","def deaccent(value):\n","  from unicodedata import normalize, combining\n","  value = normalize('NFD', value)\n","  value = u''.join(c for c in value if not combining(c))\n","  return value\n","\n","def load_document(name):\n","\tfile = open(name, mode='rt', encoding='utf-8')\n","\ttext = file.read()\n","\tfile.close()\n","\treturn text\n","\n","def sep_to_pairs(document):\n","\tlines = doc.strip().split('\\n')\n","\tpairs = [line.split('\\t') for line in  lines]\n","\treturn pairs\n","\n","def clean_all_pairs(lines):\n","\tcleaned = list()\n","\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n","\ttable = str.maketrans('', '', string.punctuation)\n","\tfor pair in lines:\n","\t\tclean_pair = list()\n","\t\tfor line in pair:\n","\t\t\t# normalize unicode characters\n","\t\t\tline = deaccent(line)#normalize('NFC', line).encode('ascii', 'ignore') \u001a\n","\t\t\t#line = line.decode('UTF-8')\n","\t\t\t# tokenize on white space\n","\t\t\tline = line.split()\n","\t\t\t# convert to lowercase\n","\t\t\tline = [word.lower() for word in line]\n","\t\t\t# remove punctuation from each token\n","\t\t\tline = [word.translate(table) for word in line]\n","\t\t\t# remove non-printable chars form each token\n","\t\t\t#line = [re_print.sub('', w) for w in line]\n","\t\t\t# remove tokens with numbers in them\n","\t\t\tline = [word for word in line if word.isalpha()]\n","\t\t\t# store as string\n","\t\t\tclean_pair.append(' '.join(line))\n","\t\tcleaned.append(clean_pair)\n","\treturn array(cleaned)\n","\n","def save_data(sentences, name):\n","\tdump(sentences, open(name, 'wb'))\n","\tprint('Saved: %s' % name)\n","\n","name = 'rus.txt'\n","doc = load_document(name)\n","pairs = sep_to_pairs(doc)\n","clean_pairs = clean_all_pairs(pairs)\n","save_data(clean_pairs, 'english-russian.pkl')\n","for i in range(100):\n","\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"],"execution_count":51,"outputs":[{"output_type":"stream","text":["Saved: english-russian.pkl\n","[go] => [иди]\n","[go] => [идите]\n","[hi] => [здравствуите]\n","[hi] => [привет]\n","[hi] => [хаи]\n","[hi] => [здрасте]\n","[hi] => [здорово]\n","[run] => [беги]\n","[run] => [бегите]\n","[run] => [беги]\n","[run] => [бегите]\n","[who] => [кто]\n","[fire] => [огонь]\n","[fire] => [пожар]\n","[help] => [помогите]\n","[help] => [на помощь]\n","[help] => [спасите]\n","[jump] => [прыгаи]\n","[jump] => [прыгаите]\n","[jump] => [прыгаи]\n","[stop] => [стои]\n","[stop] => [остановитесь]\n","[stop] => [остановись]\n","[wait] => [подожди]\n","[wait] => [подождите]\n","[wait] => [ждите]\n","[wait] => [жди]\n","[wait] => [ждите]\n","[do it] => [сделаи это]\n","[go on] => [продолжаи]\n","[go on] => [продолжаите]\n","[hello] => [здравствуите]\n","[hello] => [привет]\n","[hello] => [алло]\n","[hurry] => [поспешите]\n","[i ran] => [я бежал]\n","[i ran] => [я бежала]\n","[i ran] => [я побежал]\n","[i ran] => [я побежала]\n","[i see] => [понимаю]\n","[i see] => [понятно]\n","[i see] => [вижу]\n","[i try] => [я пытаюсь]\n","[i try] => [я стараюсь]\n","[i try] => [я пробую]\n","[i won] => [я победил]\n","[i won] => [я победила]\n","[i won] => [я выиграл]\n","[i won] => [я выиграла]\n","[oh no] => [о нет]\n","[relax] => [попустись]\n","[smile] => [улыбочка]\n","[smile] => [улыбнитесь]\n","[smile] => [улыбнись]\n","[smile] => [улыбаися]\n","[smile] => [улыбнитесь]\n","[smile] => [улыбаитесь]\n","[attack] => [в атаку]\n","[cheers] => [за ваше здоровье]\n","[cheers] => [за ваше здоровье]\n","[cheers] => [будем]\n","[cheers] => [ваше здоровье]\n","[cheers] => [твое здоровье]\n","[eat it] => [съешь это]\n","[eat up] => [доедаи]\n","[freeze] => [ни с места]\n","[freeze] => [застынь]\n","[freeze] => [замри]\n","[get up] => [вставаи]\n","[get up] => [поднимаися]\n","[get up] => [поднимаитесь]\n","[go now] => [а теперь уходи]\n","[go now] => [идите сеичас]\n","[go now] => [поезжаите сеичас]\n","[go now] => [иди сеичас]\n","[go now] => [поезжаи сеичас]\n","[go now] => [теперь иди]\n","[go now] => [теперь идите]\n","[go now] => [иди уже]\n","[go now] => [идите уже]\n","[got it] => [понял]\n","[got it] => [усек]\n","[got it] => [усекла]\n","[got it] => [ферштеин]\n","[he ran] => [он бежал]\n","[hop in] => [залезаи]\n","[hop in] => [запрыгиваи]\n","[hug me] => [обними меня]\n","[i fell] => [я упал]\n","[i knit] => [я вяжу]\n","[i know] => [я знаю]\n","[i know] => [знамо дело]\n","[i know] => [знаю]\n","[i left] => [я ушел]\n","[i left] => [я ушла]\n","[i left] => [я уехал]\n","[i left] => [я уехала]\n","[i lied] => [я солгал]\n","[i lied] => [я соврал]\n","[i lost] => [я проиграл]\n"],"name":"stdout"}]},{"metadata":{"id":"NKPgfdbY6P-J","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":68},"outputId":"5f3a638d-a12d-416f-9dfb-bd8a648eaef3","executionInfo":{"status":"ok","timestamp":1527364948613,"user_tz":-180,"elapsed":1496,"user":{"displayName":"Виктор Кириллович Борзов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102444268362895471857"}}},"cell_type":"code","source":["from pickle import load\n","from pickle import dump\n","from numpy.random import rand\n","from numpy.random import shuffle\n","\n","# load a clean dataset\n","def load_clean_sentences(filename):\n","\treturn load(open(filename, 'rb'))\n","\n","# save a list of clean sentences to file\n","def save_clean_data(sentences, filename):\n","\tdump(sentences, open(filename, 'wb'))\n","\tprint('Saved: %s' % filename)\n","\n","# load dataset\n","raw_dataset = load_clean_sentences('english-russian.pkl')\n","\n","# reduce dataset size\n","n_sentences = 10000\n","dataset = raw_dataset[:n_sentences, :]\n","# random shuffle\n","shuffle(dataset)\n","# split into train/test\n","train, test = dataset[:9000], dataset[9000:]\n","# save\n","save_clean_data(dataset, 'english-russian-both.pkl')\n","save_clean_data(train, 'english-russian-train.pkl')\n","save_clean_data(test, 'english-russian-test.pkl')"],"execution_count":52,"outputs":[{"output_type":"stream","text":["Saved: english-russian-both.pkl\n","Saved: english-russian-train.pkl\n","Saved: english-russian-test.pkl\n"],"name":"stdout"}]},{"metadata":{"id":"Bjv_NlOA5Rrx","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":2875},"outputId":"e315c967-6425-492b-e228-5d8e06f3bdb1","executionInfo":{"status":"ok","timestamp":1527365210373,"user_tz":-180,"elapsed":243669,"user":{"displayName":"Виктор Кириллович Борзов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102444268362895471857"}}},"cell_type":"code","source":["##########-MODEL-##########\n","!apt-get -qq install -y graphviz && pip install -q pydot\n","import pydot\n","\n","from pickle import load\n","from numpy import array\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.utils.vis_utils import plot_model\n","from keras.models import Sequential\n","from keras.layers import LSTM\n","from keras.layers import Dense\n","from keras.layers import Embedding\n","from keras.layers import RepeatVector\n","from keras.layers import TimeDistributed\n","from keras.callbacks import ModelCheckpoint\n","\n","def load_clean_sentences(filename):\n","\treturn load(open(filename, 'rb'))\n","\n","def create_new_tokenizer(lines):\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer\n","\n","def max_length_in_list(lines):\n","\treturn max(len(line.split()) for line in lines)\n","\n","def encode_sequences(tokenizer, length, lines):\n","\t# encode sequences to integer \n","\tX = tokenizer.texts_to_sequences(lines)\n","\t# pad sequences with 0 values\n","\tX = pad_sequences(X, maxlen=length, padding='post')\n","\treturn X\n","\n","def encode_output(sequences, vocab_size):\n","\tylist = list()\n","\tfor sequence in sequences:\n","\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n","\t\tylist.append(encoded)\n","\ty = array(ylist)\n","\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n","\treturn y\n","\n","def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n","\tmodel = Sequential()\n","\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n","\tmodel.add(LSTM(n_units))\n","\tmodel.add(RepeatVector(tar_timesteps))\n","\tmodel.add(LSTM(n_units, return_sequences=True))\n","\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n","\treturn model\n","\n","dataset = load_clean_sentences('english-russian-both.pkl')\n","trainset = load_clean_sentences('english-russian-train.pkl')\n","testset = load_clean_sentences('english-russian-test.pkl')\n","\n","eng_tokenizer = create_new_tokenizer(dataset[:, 0])\n","eng_vocab_size = len(eng_tokenizer.word_index) + 1\n","eng_length = max_length_in_list(dataset[:, 0])\n","print('English Vocabulary Size: %d' % eng_vocab_size)\n","print('English Max Length: %d' % (eng_length))\n","rus_tokenizer = create_new_tokenizer(dataset[:, 1])\n","rus_vocab_size = len(rus_tokenizer.word_index) + 1\n","rus_length = max_length_in_list(dataset[:, 1])\n","print('Russian Vocabulary Size: %d' % rus_vocab_size)\n","print('Russian Max Length: %d' % (rus_length))\n","\n","trainX = encode_sequences(rus_tokenizer, rus_length, trainset[:, 1])\n","trainY = encode_sequences(eng_tokenizer, eng_length, trainset[:, 0])\n","trainY = encode_output(trainY, eng_vocab_size)\n","\n","testX = encode_sequences(rus_tokenizer, rus_length, testset[:, 1])\n","testY = encode_sequences(eng_tokenizer, eng_length, testset[:, 0])\n","testY = encode_output(testY, eng_vocab_size)\n","\n","model = define_model(rus_vocab_size, eng_vocab_size, rus_length, eng_length, 256)\n","model.compile(optimizer='ADAM', loss='categorical_crossentropy')\n","print(model.summary())\n","# fit model\n","filename = 'model.h5'\n","checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","model.fit(trainX, trainY, epochs=35, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=20)\n","outputs = [layer.output for layer in model.layers]  \n","print(outputs)"],"execution_count":53,"outputs":[{"output_type":"stream","text":["English Vocabulary Size: 1838\n","English Max Length: 5\n","Russian Vocabulary Size: 4733\n","Russian Max Length: 8\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_4 (Embedding)      (None, 8, 256)            1211648   \n","_________________________________________________________________\n","lstm_7 (LSTM)                (None, 256)               525312    \n","_________________________________________________________________\n","repeat_vector_4 (RepeatVecto (None, 5, 256)            0         \n","_________________________________________________________________\n","lstm_8 (LSTM)                (None, 5, 256)            525312    \n","_________________________________________________________________\n","time_distributed_4 (TimeDist (None, 5, 1838)           472366    \n","=================================================================\n","Total params: 2,734,638\n","Trainable params: 2,734,638\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Train on 9000 samples, validate on 1000 samples\n","Epoch 1/35\n","\n","Epoch 00001: val_loss improved from inf to 3.11587, saving model to model.h5\n","Epoch 2/35\n","\n","Epoch 00002: val_loss improved from 3.11587 to 2.94187, saving model to model.h5\n","Epoch 3/35\n","\n","Epoch 00003: val_loss improved from 2.94187 to 2.82994, saving model to model.h5\n","Epoch 4/35\n","\n","Epoch 00004: val_loss improved from 2.82994 to 2.73466, saving model to model.h5\n","Epoch 5/35\n","\n","Epoch 00005: val_loss improved from 2.73466 to 2.66100, saving model to model.h5\n","Epoch 6/35\n","\n","Epoch 00006: val_loss improved from 2.66100 to 2.57219, saving model to model.h5\n","Epoch 7/35\n","\n","Epoch 00007: val_loss improved from 2.57219 to 2.46543, saving model to model.h5\n","Epoch 8/35\n","\n","Epoch 00008: val_loss improved from 2.46543 to 2.35194, saving model to model.h5\n","Epoch 9/35\n","\n","Epoch 00009: val_loss improved from 2.35194 to 2.23916, saving model to model.h5\n","Epoch 10/35\n","\n","Epoch 00010: val_loss improved from 2.23916 to 2.16450, saving model to model.h5\n","Epoch 11/35\n","\n","Epoch 00011: val_loss improved from 2.16450 to 2.08504, saving model to model.h5\n","Epoch 12/35\n","\n","Epoch 00012: val_loss improved from 2.08504 to 2.02162, saving model to model.h5\n","Epoch 13/35\n","\n","Epoch 00013: val_loss improved from 2.02162 to 1.96501, saving model to model.h5\n","Epoch 14/35\n","\n","Epoch 00014: val_loss improved from 1.96501 to 1.91478, saving model to model.h5\n","Epoch 15/35\n","\n","Epoch 00015: val_loss improved from 1.91478 to 1.87625, saving model to model.h5\n","Epoch 16/35\n","\n","Epoch 00016: val_loss improved from 1.87625 to 1.83573, saving model to model.h5\n","Epoch 17/35\n","\n","Epoch 00017: val_loss improved from 1.83573 to 1.80103, saving model to model.h5\n","Epoch 18/35\n","\n","Epoch 00018: val_loss improved from 1.80103 to 1.77222, saving model to model.h5\n","Epoch 19/35\n","\n","Epoch 00019: val_loss improved from 1.77222 to 1.73956, saving model to model.h5\n","Epoch 20/35\n","\n","Epoch 00020: val_loss improved from 1.73956 to 1.72745, saving model to model.h5\n","Epoch 21/35\n","\n","Epoch 00021: val_loss improved from 1.72745 to 1.70414, saving model to model.h5\n","Epoch 22/35\n","\n","Epoch 00022: val_loss improved from 1.70414 to 1.67688, saving model to model.h5\n","Epoch 23/35\n","\n","Epoch 00023: val_loss did not improve from 1.67688\n","Epoch 24/35\n","\n","Epoch 00024: val_loss improved from 1.67688 to 1.67264, saving model to model.h5\n","Epoch 25/35\n","\n","Epoch 00025: val_loss improved from 1.67264 to 1.66457, saving model to model.h5\n","Epoch 26/35\n","\n","Epoch 00026: val_loss improved from 1.66457 to 1.66091, saving model to model.h5\n","Epoch 27/35\n","\n","Epoch 00027: val_loss improved from 1.66091 to 1.65420, saving model to model.h5\n","Epoch 28/35\n","\n","Epoch 00028: val_loss did not improve from 1.65420\n","Epoch 29/35\n","\n","Epoch 00029: val_loss did not improve from 1.65420\n","Epoch 30/35\n","\n","Epoch 00030: val_loss did not improve from 1.65420\n","Epoch 31/35\n","\n","Epoch 00031: val_loss did not improve from 1.65420\n","Epoch 32/35\n","\n","Epoch 00032: val_loss did not improve from 1.65420\n","Epoch 33/35\n","\n","Epoch 00033: val_loss did not improve from 1.65420\n","Epoch 34/35\n","\n","Epoch 00034: val_loss did not improve from 1.65420\n","Epoch 35/35\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Epoch 00035: val_loss did not improve from 1.65420\n","[<tf.Tensor 'embedding_4/GatherV2:0' shape=(?, 8, 256) dtype=float32>, <tf.Tensor 'lstm_7/TensorArrayReadV3:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'repeat_vector_4/Tile:0' shape=(?, 5, 256) dtype=float32>, <tf.Tensor 'lstm_8/transpose_1:0' shape=(?, ?, 256) dtype=float32>, <tf.Tensor 'time_distributed_4/Reshape_1:0' shape=(?, 5, 1838) dtype=float32>]\n"],"name":"stdout"}]},{"metadata":{"id":"3iLe6wkGLMk2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":785},"outputId":"37006690-dec5-4286-8c9d-21bd6bc8dcf6","executionInfo":{"status":"ok","timestamp":1527365474565,"user_tz":-180,"elapsed":162681,"user":{"displayName":"Виктор Кириллович Борзов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102444268362895471857"}}},"cell_type":"code","source":["##########-EVALUATION-##########\n","from pickle import load\n","from numpy import array\n","from numpy import argmax\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import load_model\n","from nltk.translate.bleu_score import corpus_bleu\n","from nltk.translate.bleu_score import SmoothingFunction\n","\n","\n","chencherry = SmoothingFunction()\n","def load_clean_sentences(filename):\n","\treturn load(open(filename, 'rb'))\n","\n","def create_tokenizer(lines):\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer\n","\n","def max_length(lines):\n","\treturn max(len(line.split()) for line in lines)\n","\n","def encode_sequences(tokenizer, length, lines):\n","\tX = tokenizer.texts_to_sequences(lines)\n","\tX = pad_sequences(X, maxlen=length, padding='post')\n","\treturn X\n","\n","def word_for_id(integer, tokenizer):\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == integer:\n","\t\t\treturn word\n","\treturn None\n","\n","def predict_sequence(model, tokenizer, source):\n","\tprediction = model.predict(source, verbose=0)[0]\n","\tintegers = [argmax(vector) for vector in prediction]\n","\ttarget = list()\n","\tfor i in integers:\n","\t\tword = word_for_id(i, tokenizer)\n","\t\tif word is None:\n","\t\t\tbreak\n","\t\ttarget.append(word)\n","\treturn ' '.join(target)\n","\n","def evaluate_model(model, tokenizer, sources, raw_dataset):\n","\tactual, predicted = list(), list()\n","\tfor i, source in enumerate(sources):\n","\t\tsource = source.reshape((1, source.shape[0]))\n","\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n","\t\traw_target, raw_src = raw_dataset[i]\n","\t\tif i < 10:\n","\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n","\t\tactual.append(raw_target.split())\n","\t\tpredicted.append(translation.split())\n","\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n","\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n","\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n","\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n","\n","dataset = load_clean_sentences('english-russian-both.pkl')\n","trainset = load_clean_sentences('english-russian-train.pkl')\n","testset = load_clean_sentences('english-russian-test.pkl')\n","eng_tokenizer = create_tokenizer(dataset[:, 0])\n","eng_vocab_size = len(eng_tokenizer.word_index) + 1\n","eng_length = max_length(dataset[:, 0])\n","rus_tokenizer = create_tokenizer(dataset[:, 1])\n","rus_vocab_size = len(rus_tokenizer.word_index) + 1\n","rus_length = max_length(dataset[:, 1])\n","trainX = encode_sequences(rus_tokenizer, rus_length, trainset[:, 1])\n","testX = encode_sequences(rus_tokenizer, rus_length, testset[:, 1])\n","model = load_model('model.h5')\n","print('train')\n","evaluate_model(model, eng_tokenizer, trainX, trainset)\n","print('test')\n","evaluate_model(model, eng_tokenizer, testX, testset)"],"execution_count":54,"outputs":[{"output_type":"stream","text":["train\n","src=[он не умеет петь], target=[he cant sing], predicted=[he cant sing]\n","src=[перестаньте снимать], target=[stop filming], predicted=[stop filming]\n","src=[том болеет], target=[tom is sick], predicted=[tom is]\n","src=[вы устали], target=[youre tired], predicted=[youre tired]\n","src=[не открываите ее], target=[dont open it], predicted=[dont open it]\n","src=[я твои], target=[im yours], predicted=[im yours]\n","src=[сделаите еще раз], target=[do that again], predicted=[do that again]\n","src=[ешьте не спеша], target=[eat slowly], predicted=[eat slowly]\n","src=[будь объективен], target=[be objective], predicted=[be objective]\n","src=[мы с тобои ровесники], target=[im your age], predicted=[im your age]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"},{"output_type":"stream","text":["BLEU-1: 0.090809\n","BLEU-2: 0.286736\n","BLEU-3: 0.454176\n","BLEU-4: 0.509518\n","test\n","src=[увидимся], target=[see you], predicted=[are you then]\n","src=[жизнь забавна], target=[life is fun], predicted=[mary is hard]\n","src=[они обожглись], target=[they burned], predicted=[theyre boys]\n","src=[теперь уходи], target=[now go away], predicted=[now get out]\n","src=[идите без меня], target=[go without me], predicted=[come without me]\n","src=[прекрати пожалуиста], target=[please stop], predicted=[please stop]\n","src=[это порок], target=[its a vice], predicted=[its is]\n","src=[просто будь счастлив], target=[just be happy], predicted=[just be happy]\n","src=[я приготовила ужин], target=[i made supper], predicted=[i made dinner]\n","src=[у меня диабет], target=[im diabetic], predicted=[i am diabetic]\n","BLEU-1: 0.081189\n","BLEU-2: 0.270323\n","BLEU-3: 0.437357\n","BLEU-4: 0.493259\n"],"name":"stdout"}]}]}