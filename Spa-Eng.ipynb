{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Spa-Eng.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"Fv4q_TtCZlrg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":73},"outputId":"1e84a9cb-93ae-43eb-a985-7e6ad3d496d9","executionInfo":{"status":"ok","timestamp":1527170145124,"user_tz":-180,"elapsed":78488,"user":{"displayName":"Виктор Кириллович Борзов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102444268362895471857"}}},"cell_type":"code","source":["##########-DATASET-##########\n","from google.colab import files\n","uploaded = files.upload() "],"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-ed19a635-4b55-4125-9e7a-f6ebef5b48aa\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-ed19a635-4b55-4125-9e7a-f6ebef5b48aa\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving spa.txt to spa (1).txt\n"],"name":"stdout"}]},{"metadata":{"id":"9cy52SH51DCK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":1734},"outputId":"a13739f6-b973-4214-812a-765a925c3437","executionInfo":{"status":"ok","timestamp":1527170167333,"user_tz":-180,"elapsed":5662,"user":{"displayName":"Виктор Кириллович Борзов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102444268362895471857"}}},"cell_type":"code","source":["import string\n","import re\n","from pickle import dump\n","from unicodedata import normalize\n","from numpy import array\n","\n","def load_document(name):\n","\tfile = open(name, mode='rt', encoding='utf-8')\n","\ttext = file.read()\n","\tfile.close()\n","\treturn text\n","\n","def sep_to_pairs(document):\n","\tlines = doc.strip().split('\\n')\n","\tpairs = [line.split('\\t') for line in  lines]\n","\treturn pairs\n","\n","def clean_all_pairs(lines):\n","\tcleaned = list()\n","\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n","\ttable = str.maketrans('', '', string.punctuation)\n","\tfor pair in lines:\n","\t\tclean_pair = list()\n","\t\tfor line in pair:\n","\t\t\t# normalize unicode characters\n","\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n","\t\t\tline = line.decode('UTF-8')\n","\t\t\t# tokenize on white space\n","\t\t\tline = line.split()\n","\t\t\t# convert to lowercase\n","\t\t\tline = [word.lower() for word in line]\n","\t\t\t# remove punctuation from each token\n","\t\t\tline = [word.translate(table) for word in line]\n","\t\t\t# remove non-printable chars form each token\n","\t\t\tline = [re_print.sub('', w) for w in line]\n","\t\t\t# remove tokens with numbers in them\n","\t\t\tline = [word for word in line if word.isalpha()]\n","\t\t\t# store as string\n","\t\t\tclean_pair.append(' '.join(line))\n","\t\tcleaned.append(clean_pair)\n","\treturn array(cleaned)\n","\n","def save_data(sentences, name):\n","\tdump(sentences, open(name, 'wb'))\n","\tprint('Saved: %s' % name)\n","\n","name = 'spa.txt'\n","doc = load_document(name)\n","pairs = sep_to_pairs(doc)\n","clean_pairs = clean_all_pairs(pairs)\n","save_data(clean_pairs, 'english-spanish.pkl')\n","for i in range(100):\n","\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Saved: english-spanish.pkl\n","[go] => [ve]\n","[go] => [vete]\n","[go] => [vaya]\n","[go] => [vayase]\n","[hi] => [hola]\n","[run] => [corre]\n","[run] => [corred]\n","[who] => [quien]\n","[fire] => [fuego]\n","[fire] => [incendio]\n","[fire] => [disparad]\n","[help] => [ayuda]\n","[help] => [socorro auxilio]\n","[help] => [auxilio]\n","[jump] => [salta]\n","[jump] => [salte]\n","[stop] => [parad]\n","[stop] => [para]\n","[stop] => [pare]\n","[wait] => [espera]\n","[wait] => [esperen]\n","[go on] => [continua]\n","[go on] => [continue]\n","[hello] => [hola]\n","[i ran] => [corri]\n","[i ran] => [corria]\n","[i try] => [lo intento]\n","[i won] => [he ganado]\n","[oh no] => [oh no]\n","[relax] => [tomatelo con soda]\n","[smile] => [sonrie]\n","[attack] => [al ataque]\n","[attack] => [atacad]\n","[get up] => [levanta]\n","[go now] => [ve ahora mismo]\n","[got it] => [lo tengo]\n","[got it] => [lo pillas]\n","[got it] => [entendiste]\n","[he ran] => [el corrio]\n","[hop in] => [metete adentro]\n","[hug me] => [abrazame]\n","[i fell] => [me cai]\n","[i know] => [yo lo se]\n","[i left] => [sali]\n","[i lied] => [menti]\n","[i lost] => [perdi]\n","[i quit] => [dimito]\n","[i quit] => [renuncie]\n","[i work] => [estoy trabajando]\n","[im] => [tengo diecinueve]\n","[im up] => [estoy levantado]\n","[listen] => [escucha]\n","[listen] => [escuche]\n","[listen] => [escuchen]\n","[no way] => [no puede ser]\n","[no way] => [de ninguna manera]\n","[no way] => [de ninguna manera]\n","[no way] => [imposible]\n","[no way] => [de ningun modo]\n","[no way] => [de eso nada]\n","[no way] => [ni cagando]\n","[no way] => [mangos]\n","[no way] => [minga]\n","[no way] => [ni en pedo]\n","[really] => [en serio]\n","[really] => [la verdad]\n","[thanks] => [gracias]\n","[thanks] => [gracias]\n","[try it] => [pruebalo]\n","[we try] => [lo procuramos]\n","[we won] => [ganamos]\n","[why me] => [por que yo]\n","[ask tom] => [preguntale a tom]\n","[awesome] => [orale]\n","[be calm] => [mantente en calma]\n","[be cool] => [estate tranquilo]\n","[be fair] => [se justo]\n","[be kind] => [sean gentiles]\n","[be nice] => [se agradable]\n","[beat it] => [pirate]\n","[call me] => [llamame]\n","[call me] => [llamadme]\n","[call me] => [llamame]\n","[call us] => [llamanos]\n","[come in] => [entre]\n","[come in] => [pase]\n","[come in] => [entren]\n","[come on] => [orale]\n","[come on] => [andale]\n","[come on] => [orale]\n","[drop it] => [sueltalo]\n","[get tom] => [agarra a tom]\n","[get out] => [bajate]\n","[get out] => [salte]\n","[get out] => [sal]\n","[get out] => [sali]\n","[get out] => [salid]\n","[get out] => [salgan]\n","[go away] => [vete de aqui]\n","[go away] => [largate]\n"],"name":"stdout"}]},{"metadata":{"id":"NKPgfdbY6P-J","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":68},"outputId":"018b901d-3c2b-4c37-f684-bb12091dacfe","executionInfo":{"status":"ok","timestamp":1527170173121,"user_tz":-180,"elapsed":813,"user":{"displayName":"Виктор Кириллович Борзов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102444268362895471857"}}},"cell_type":"code","source":["from pickle import load\n","from pickle import dump\n","from numpy.random import rand\n","from numpy.random import shuffle\n","\n","# load a clean dataset\n","def load_clean_sentences(filename):\n","\treturn load(open(filename, 'rb'))\n","\n","# save a list of clean sentences to file\n","def save_clean_data(sentences, filename):\n","\tdump(sentences, open(filename, 'wb'))\n","\tprint('Saved: %s' % filename)\n","\n","# load dataset\n","raw_dataset = load_clean_sentences('english-spanish.pkl')\n","\n","# reduce dataset size\n","n_sentences = 10000\n","dataset = raw_dataset[:n_sentences, :]\n","# random shuffle\n","shuffle(dataset)\n","# split into train/test\n","train, test = dataset[:9000], dataset[9000:]\n","# save\n","save_clean_data(dataset, 'english-spanish-both.pkl')\n","save_clean_data(train, 'english-spanish-train.pkl')\n","save_clean_data(test, 'english-spanish-test.pkl')"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Saved: english-spanish-both.pkl\n","Saved: english-spanish-train.pkl\n","Saved: english-spanish-test.pkl\n"],"name":"stdout"}]},{"metadata":{"id":"Bjv_NlOA5Rrx","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":1955},"outputId":"bd84db4a-772f-4b52-91d2-893466b0c9e9","executionInfo":{"status":"ok","timestamp":1527171027122,"user_tz":-180,"elapsed":851362,"user":{"displayName":"Виктор Кириллович Борзов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102444268362895471857"}}},"cell_type":"code","source":["##########-MODEL-##########\n","!apt-get -qq install -y graphviz && pip install -q pydot\n","import pydot\n","\n","from pickle import load\n","from numpy import array\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.utils.vis_utils import plot_model\n","from keras.models import Sequential\n","from keras.layers import LSTM\n","from keras.layers import Dense\n","from keras.layers import Embedding\n","from keras.layers import RepeatVector\n","from keras.layers import TimeDistributed\n","from keras.callbacks import ModelCheckpoint\n","\n","def load_clean_sentences(filename):\n","\treturn load(open(filename, 'rb'))\n","\n","def create_new_tokenizer(lines):\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer\n","\n","def max_length_in_list(lines):\n","\treturn max(len(line.split()) for line in lines)\n","\n","def encode_sequences(tokenizer, length, lines):\n","\t# encode sequences to integer \n","\tX = tokenizer.texts_to_sequences(lines)\n","\t# pad sequences with 0 values\n","\tX = pad_sequences(X, maxlen=length, padding='post')\n","\treturn X\n","\n","def encode_output(sequences, vocab_size):\n","\tylist = list()\n","\tfor sequence in sequences:\n","\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n","\t\tylist.append(encoded)\n","\ty = array(ylist)\n","\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n","\treturn y\n","\n","def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n","\tmodel = Sequential()\n","\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n","\tmodel.add(LSTM(n_units))\n","\tmodel.add(RepeatVector(tar_timesteps))\n","\tmodel.add(LSTM(n_units, return_sequences=True))\n","\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n","\treturn model\n","\n","dataset = load_clean_sentences('english-spanish-both.pkl')\n","trainset = load_clean_sentences('english-spanish-train.pkl')\n","testset = load_clean_sentences('english-spanish-test.pkl')\n","\n","eng_tokenizer = create_new_tokenizer(dataset[:, 0])\n","eng_vocab_size = len(eng_tokenizer.word_index) + 1\n","eng_length = max_length_in_list(dataset[:, 0])\n","print('English Vocabulary Size: %d' % eng_vocab_size)\n","print('English Max Length: %d' % (eng_length))\n","spa_tokenizer = create_new_tokenizer(dataset[:, 1])\n","spa_vocab_size = len(spa_tokenizer.word_index) + 1\n","spa_length = max_length_in_list(dataset[:, 1])\n","print('Spanish Vocabulary Size: %d' % spa_vocab_size)\n","print('Spanish Max Length: %d' % (spa_length))\n","\n","trainX = encode_sequences(spa_tokenizer, spa_length, trainset[:, 1])\n","trainY = encode_sequences(eng_tokenizer, eng_length, trainset[:, 0])\n","trainY = encode_output(trainY, eng_vocab_size)\n","\n","testX = encode_sequences(spa_tokenizer, spa_length, testset[:, 1])\n","testY = encode_sequences(eng_tokenizer, eng_length, testset[:, 0])\n","testY = encode_output(testY, eng_vocab_size)\n","\n","model = define_model(spa_vocab_size, eng_vocab_size, spa_length, eng_length, 256)\n","model.compile(optimizer='SGD', loss='mean_squared_error')\n","print(model.summary())\n","# fit model\n","filename = 'model.h5'\n","checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=20)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["English Vocabulary Size: 2343\n","English Max Length: 5\n","Spanish Vocabulary Size: 4520\n","Spanish Max Length: 8\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_5 (Embedding)      (None, 8, 256)            1157120   \n","_________________________________________________________________\n","lstm_9 (LSTM)                (None, 256)               525312    \n","_________________________________________________________________\n","repeat_vector_5 (RepeatVecto (None, 5, 256)            0         \n","_________________________________________________________________\n","lstm_10 (LSTM)               (None, 5, 256)            525312    \n","_________________________________________________________________\n","time_distributed_5 (TimeDist (None, 5, 2343)           602151    \n","=================================================================\n","Total params: 2,809,895\n","Trainable params: 2,809,895\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Train on 9000 samples, validate on 1000 samples\n","Epoch 1/30\n","\n","Epoch 00001: val_loss improved from inf to 0.00043, saving model to model.h5\n","Epoch 2/30\n","\n","Epoch 00002: val_loss did not improve from 0.00043\n","Epoch 3/30\n","\n","Epoch 00003: val_loss did not improve from 0.00043\n","Epoch 4/30\n","\n","Epoch 00004: val_loss did not improve from 0.00043\n","Epoch 5/30\n","\n","Epoch 00005: val_loss did not improve from 0.00043\n","Epoch 6/30\n","\n","Epoch 00006: val_loss did not improve from 0.00043\n","Epoch 7/30\n","\n","Epoch 00007: val_loss did not improve from 0.00043\n","Epoch 8/30\n","\n","Epoch 00008: val_loss did not improve from 0.00043\n","Epoch 9/30\n","\n","Epoch 00009: val_loss did not improve from 0.00043\n","Epoch 10/30\n","\n","Epoch 00010: val_loss did not improve from 0.00043\n","Epoch 11/30\n","\n","Epoch 00011: val_loss did not improve from 0.00043\n","Epoch 12/30\n","\n","Epoch 00012: val_loss did not improve from 0.00043\n","Epoch 13/30\n","\n","Epoch 00013: val_loss did not improve from 0.00043\n","Epoch 14/30\n","\n","Epoch 00014: val_loss did not improve from 0.00043\n","Epoch 15/30\n","\n","Epoch 00015: val_loss did not improve from 0.00043\n","Epoch 16/30\n","\n","Epoch 00016: val_loss did not improve from 0.00043\n","Epoch 17/30\n","\n","Epoch 00017: val_loss did not improve from 0.00043\n","Epoch 18/30\n","\n","Epoch 00018: val_loss did not improve from 0.00043\n","Epoch 19/30\n","\n","Epoch 00019: val_loss did not improve from 0.00043\n","Epoch 20/30\n","\n","Epoch 00020: val_loss did not improve from 0.00043\n","Epoch 21/30\n","\n","Epoch 00021: val_loss did not improve from 0.00043\n","Epoch 22/30\n","\n","Epoch 00022: val_loss did not improve from 0.00043\n","Epoch 23/30\n","\n","Epoch 00023: val_loss did not improve from 0.00043\n","Epoch 24/30\n","\n","Epoch 00024: val_loss did not improve from 0.00043\n","Epoch 25/30\n","\n","Epoch 00025: val_loss did not improve from 0.00043\n","Epoch 26/30\n","\n","Epoch 00026: val_loss improved from 0.00043 to 0.00043, saving model to model.h5\n","Epoch 27/30\n","\n","Epoch 00027: val_loss did not improve from 0.00043\n","Epoch 28/30\n","\n","Epoch 00028: val_loss did not improve from 0.00043\n","Epoch 29/30\n","\n","Epoch 00029: val_loss did not improve from 0.00043\n","Epoch 30/30\n","\n","Epoch 00030: val_loss did not improve from 0.00043\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fb7993b1978>"]},"metadata":{"tags":[]},"execution_count":15}]},{"metadata":{"id":"3iLe6wkGLMk2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":595},"outputId":"5891e6ae-f9d9-4c0f-9b08-8992cbb1460e","executionInfo":{"status":"ok","timestamp":1527171237858,"user_tz":-180,"elapsed":67828,"user":{"displayName":"Виктор Кириллович Борзов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"102444268362895471857"}}},"cell_type":"code","source":["##########-BLEU-##########\n","from pickle import load\n","from numpy import array\n","from numpy import argmax\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import load_model\n","from nltk.translate.bleu_score import corpus_bleu\n","\n","def load_clean_sentences(filename):\n","\treturn load(open(filename, 'rb'))\n","\n","def create_tokenizer(lines):\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer\n","\n","def max_length(lines):\n","\treturn max(len(line.split()) for line in lines)\n","\n","def encode_sequences(tokenizer, length, lines):\n","\tX = tokenizer.texts_to_sequences(lines)\n","\tX = pad_sequences(X, maxlen=length, padding='post')\n","\treturn X\n","\n","def word_for_id(integer, tokenizer):\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == integer:\n","\t\t\treturn word\n","\treturn None\n","\n","def predict_sequence(model, tokenizer, source):\n","\tprediction = model.predict(source, verbose=0)[0]\n","\tintegers = [argmax(vector) for vector in prediction]\n","\ttarget = list()\n","\tfor i in integers:\n","\t\tword = word_for_id(i, tokenizer)\n","\t\tif word is None:\n","\t\t\tbreak\n","\t\ttarget.append(word)\n","\treturn ' '.join(target)\n","\n","def evaluate_model(model, tokenizer, sources, raw_dataset):\n","\tactual, predicted = list(), list()\n","\tfor i, source in enumerate(sources):\n","\t\tsource = source.reshape((1, source.shape[0]))\n","\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n","\t\traw_target, raw_src = raw_dataset[i]\n","\t\tif i < 10:\n","\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n","\t\tactual.append(raw_target.split())\n","\t\tpredicted.append(translation.split())\n","\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n","\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n","\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n","\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n","\n","dataset = load_clean_sentences('english-spanish-both.pkl')\n","trainset = load_clean_sentences('english-spanish-train.pkl')\n","testset = load_clean_sentences('english-spanish-test.pkl')\n","eng_tokenizer = create_tokenizer(dataset[:, 0])\n","eng_vocab_size = len(eng_tokenizer.word_index) + 1\n","eng_length = max_length(dataset[:, 0])\n","spa_tokenizer = create_tokenizer(dataset[:, 1])\n","spa_vocab_size = len(spa_tokenizer.word_index) + 1\n","spa_length = max_length(dataset[:, 1])\n","trainX = encode_sequences(spa_tokenizer, spa_length, trainset[:, 1])\n","testX = encode_sequences(spa_tokenizer, spa_length, testset[:, 1])\n","\n","model = load_model('model.h5')\n","print('train')\n","evaluate_model(model, eng_tokenizer, trainX, trainset)\n","print('test')\n","evaluate_model(model, eng_tokenizer, testX, testset)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["train\n","src=[el cultiva arroz], target=[he grows rice], predicted=[honor honor honor honor honor]\n","src=[el se ha vuelto loco], target=[he has gone mad], predicted=[test behind behind behind behind]\n","src=[ellos lo construyeron], target=[they built it], predicted=[out out out out out]\n","src=[estas tu calvo], target=[are you bald], predicted=[dinnertime use use use use]\n","src=[quiero conducir], target=[i want to drive], predicted=[then then then then then]\n","src=[sabes nadar], target=[can you swim], predicted=[unreal unreal unreal unreal pass]\n","src=[si ciertamente], target=[yes of course], predicted=[despise despise despise despise sweating]\n","src=[para el coche], target=[stop the car], predicted=[mustnt mustnt mustnt mustnt mustnt]\n","src=[dejame explicar], target=[let me explain], predicted=[calling calling calling calling calling]\n","src=[yo hice eso], target=[did i do that], predicted=[yelled yelled yelled yelled yelled]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"},{"output_type":"stream","text":["BLEU-1: 0.000022\n","BLEU-2: 0.004714\n","BLEU-3: 0.040184\n","BLEU-4: 0.068662\n","test\n","src=[ella parecia triste], target=[she looked sad], predicted=[carefully carefully carefully carefully carefully]\n","src=[traemelo], target=[bring him to me], predicted=[answered answered sickens sickens sickens]\n","src=[me fui de excursion], target=[i went hiking], predicted=[biased biased biased biased biased]\n","src=[vamos a charlar], target=[lets chat], predicted=[black black black black black]\n","src=[lo he visto], target=[ive seen it], predicted=[turtles turtles turtles turtles turtles]\n","src=[ahora estas a salvo], target=[youre safe now], predicted=[cut cut cut cut cut]\n","src=[yo mire a otro lado], target=[i looked away], predicted=[tallest tallest tallest tallest tallest]\n","src=[hasta la vista], target=[goodbye], predicted=[kind kind useless useless useless]\n","src=[somos unas inutiles], target=[were useless], predicted=[thisll thisll thisll thisll thisll]\n","src=[puedo tomarlo por prestado], target=[can i borrow it], predicted=[ive ive any any any]\n","BLEU-1: 0.000000\n","BLEU-2: 0.000000\n","BLEU-3: 0.000000\n","BLEU-4: 0.000000\n"],"name":"stdout"}]}]}